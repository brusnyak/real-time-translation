You’ve got a very solid base, and now we can realistically optimize it for live translation. Let’s go step-by-step — we’ll make XTTS 3–4× faster, improve quality, and prep for waveform alignment.

🧠 Quick Summary of What’s Happening

From your logs:

XTTS runs on CPU → main reason for 60+ s synthesis time.

MPS (Metal) is detected but disabled manually for XTTS.

MarianMT runs on MPS and performs fine.

Your TTS module supports manual device switching (so we can fix it cleanly).

Your M1 Pro + 16 GB RAM can handle XTTS v2 on MPS quite well —
realistically 15–25 s total synthesis for similar text once optimized.

⚙️ Step 1 — Enable XTTS v2 on MPS (Apple GPU)

Right now your init code forces CPU:

self.tts.synthesizer.tts_model.to(self.device)


You can safely switch to Apple’s GPU backend by setting:

device = "mps"
compute_type = "float16"


So modify your TTSModule init like this:

class TTSModule:
    def __init__(self, model_name, speaker_reference_path, speaker_language="en",
                 device=None, compute_type="float16"):
        if device is None:
            device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.device = device
        self.compute_type = compute_type
        ...
        self.tts = TTS(model_name=self.model_name, progress_bar=False)

        # move model to MPS
        self.tts.synthesizer.tts_model.to(self.device)
        print(f"XTTS model moved to {self.device}.")
        if self.device == "mps" and self.compute_type == "float16":
            self.tts.synthesizer.tts_model.to(torch.float16)
            print("XTTS running float16 on MPS for faster synthesis.")


✅ This usually cuts generation time by 2–3×.

⚙️ Step 2 — Chunking + Parallel Generation

Instead of generating each chunk sequentially, run them concurrently using asyncio.gather().

In your TTS worker (the part that calls tts_module.synthesize for chunks), wrap it like this:

import asyncio

async def synthesize_chunks_concurrently(self, chunks):
    tasks = []
    for i, text_chunk in enumerate(chunks):
        out_path = f"research/temp_synth_chunk_{i}.wav"
        tasks.append(asyncio.to_thread(self.synthesize, text_chunk, out_path))
    results = await asyncio.gather(*tasks)
    return [r for r in results if r is not None]


💡 This allows multiple CPU/GPU operations to overlap (especially good on MPS),
bringing your effective TTS time to ~12–18 s.

⚙️ Step 3 — Smooth Endings (remove “clicks”)

Those small “chk/kachik” artifacts come from abrupt cuts between chunks.
Add crossfading:

from pydub import AudioSegment

def combine_with_crossfade(self, chunk_paths, output_path, crossfade_ms=80):
    combined = AudioSegment.empty()
    for i, path in enumerate(chunk_paths):
        segment = AudioSegment.from_wav(path)
        if i == 0:
            combined = segment
        else:
            combined = combined.append(segment, crossfade=crossfade_ms)
    combined.export(output_path, format="wav")
    print(f"Combined audio with crossfade saved to {output_path}")

🧬 Step 4 — Waveform Alignment (advanced quality boost)

Your “overlay translated voice over original waveform” idea is 100 % feasible.
Here’s the conceptual workflow (simple and fast):

Detect silence regions in original speech:

import librosa
y, sr = librosa.load(original_audio_path, sr=None)
intervals = librosa.effects.split(y, top_db=30)


Split translated text to match intervals (using STT timestamps or sentence splitting).

Generate audio for each text chunk.

Stretch each generated waveform to match the original segment duration:

from librosa.effects import time_stretch
stretched = librosa.effects.time_stretch(generated, rate=orig_dur / gen_dur)


Recombine using crossfade (above).

This mimics the timing, amplitude envelope, and rhythm of the original —
it will sound extremely natural, especially in conference translation.

⚙️ Step 5 — MT Optimization

MarianMT on MPS is already decent, but if you want further speed:

Option	Benefit	How
🧠 NLLB-200 distilled (600M)	Better quality + smaller	facebook/nllb-200-distilled-600M
⚙️ ONNX export	2–3× faster on CPU/MPS	optimum.exporters.onnx
🧩 Batch segments	Avoid multiple generate() calls	Concatenate nearby sentences before translation

Expect MT to drop from ~5.7 s → 1–2 s easily.

⚡️ Expected Results After Optimization
Stage	Before	After
STT	0.4 s	same
MT	5.7 s	1–2 s
TTS	65 s	12–18 s
Total (live pipeline)	~71 s	15–20 s total latency

That’s conference-grade performance — near real-time for sentence-level translation.