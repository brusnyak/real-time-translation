import os
import time
import tempfile
import asyncio
import numpy as np
import soundfile as sf
import librosa
from pydub import AudioSegment, effects
import torch
from TTS.api import TTS
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import XttsAudioConfig
from TTS.config.shared_configs import BaseDatasetConfig
from TTS.tts.models.xtts import XttsArgs
from torch.serialization import add_safe_globals


class TTSModule:
    """
    Optimized TTS module for live speech translation.
    - Fast initialization (skip warmup, or minimal warmup)
    - Correct text-to-audio chunk alignment
    - Memory-efficient concurrent synthesis
    """

    def __init__(self, 
                 model_name="tts_models/multilingual/multi-dataset/xtts_v2",
                 speaker_reference_path=None, 
                 speaker_language="en",
                 device="cpu", 
                 compute_type="float32", 
                 sample_rate=22050,
                 skip_warmup=False):
        """
        Args:
            skip_warmup: If True, skip model warmup to save ~4s at startup
        """
        self.model_name = model_name
        self.speaker_reference_path = speaker_reference_path
        self.speaker_language = speaker_language
        self.sample_rate = sample_rate
        self.device = device
        self.compute_type = compute_type

        print(f"[TTS] Pre-loading TTS model {self.model_name}...")
        start = time.time()

        # Fix PyTorch 2.6+ weights_only security check
        add_safe_globals([XttsConfig, XttsAudioConfig, BaseDatasetConfig, XttsArgs])

        # Load TTS
        self.tts = TTS(model_name=self.model_name, progress_bar=False, gpu=False)
        
        # Move to device
        self.tts.synthesizer.tts_model.to(self.device)
        self.tts.synthesizer.tts_model.eval()

        self._load_time = time.time() - start
        print(f"[TTS] Loaded in {self._load_time:.3f}s.")

        # Speaker reference checks
        if not self.speaker_reference_path:
            print("[TTS] Warning: No speaker_reference_path. Using default voice.")
        else:
            if not os.path.exists(self.speaker_reference_path):
                raise FileNotFoundError(f"[TTS] Speaker file not found: {self.speaker_reference_path}")
            info = sf.info(self.speaker_reference_path)
            print(f"[TTS] Speaker: {info.duration:.2f}s at {info.samplerate}Hz")

        # Optional warmup (saves ~4s if skipped)
        if not skip_warmup:
            print("[TTS] Warming up model...")
            try:
                _ = self.tts.tts(
                    text="Hello.",
                    speaker_wav=self.speaker_reference_path,
                    language=self.speaker_language
                )
                print("[TTS] Warmup done.")
            except Exception as e:
                print(f"[TTS] Warmup failed (non-critical): {e}")

    async def synthesize_chunks_concurrently(self, text_chunks, crossfade_ms=50):
        """
        Synthesize text chunks concurrently.
        
        Args:
            text_chunks: List of text strings to synthesize
            crossfade_ms: Crossfade duration in milliseconds
        
        Returns:
            Path to combined audio file, or None if failed
        """
        if not text_chunks:
            return None

        print(f"[TTS] Synthesizing {len(text_chunks)} chunks...")
        
        # Launch synthesis tasks in thread pool
        tasks = [asyncio.to_thread(self._synthesize_chunk, txt) for txt in text_chunks]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter valid results
        valid_chunks = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"[TTS] Chunk {i} failed: {result}")
                continue
            if result is None:
                print(f"[TTS] Chunk {i} returned None")
                continue
            
            audio_np, sr = result
            if audio_np.size == 0:
                print(f"[TTS] Chunk {i} is empty")
                continue
            
            valid_chunks.append((audio_np, sr))

        if not valid_chunks:
            print("[TTS] No valid chunks produced")
            return None

        # Combine chunks with crossfade
        out_path = self._combine_chunks(valid_chunks, crossfade_ms)
        print(f"[TTS] Combined {len(valid_chunks)} chunks to {out_path}")
        return out_path

    def _synthesize_chunk(self, text, temperature=0.3, speed=1.0):
        """
        Synchronous synthesis of a single text chunk.
        Runs in thread pool via asyncio.to_thread.
        
        Returns:
            (audio_np, sample_rate) or None on failure
        """
        try:
            # Generate audio using TTS
            audio_output = self.tts.tts(
                text=text,
                speaker_wav=self.speaker_reference_path,
                language=self.speaker_language,
                temperature=temperature,
                speed=speed
            )

            # Handle different return types from TTS
            audio_np = self._extract_audio(audio_output)
            
            if audio_np is None or audio_np.size == 0:
                print(f"[TTS] Empty audio for: '{text[:50]}'")
                return None

            # Ensure float32
            audio_np = audio_np.astype(np.float32)
            
            # Clip to [-1, 1] to prevent audio artifacts
            audio_np = np.clip(audio_np, -1.0, 1.0)
            
            return audio_np, self.sample_rate

        except Exception as e:
            print(f"[TTS] Synthesis failed for '{text[:50]}': {e}")
            return None

    def _extract_audio(self, output):
        """
        Extract numpy array from various TTS return types.
        Handles different Coqui TTS versions.
        """
        if isinstance(output, np.ndarray):
            return output
        elif isinstance(output, dict) and "wav" in output:
            wav = output["wav"]
            if isinstance(wav, np.ndarray):
                return wav
            elif isinstance(wav, list):
                return np.array(wav, dtype=np.float32)
        elif isinstance(output, list):
            if all(isinstance(x, (float, int, np.floating)) for x in output):
                return np.array(output, dtype=np.float32)
            elif all(isinstance(x, np.ndarray) for x in output):
                non_empty = [x for x in output if x.size > 0]
                if non_empty:
                    return np.concatenate(non_empty)
        elif isinstance(output, str) and os.path.exists(output):
            # If TTS returned a file path, load it
            audio_np, _ = sf.read(output, dtype='float32')
            return audio_np
        
        return None

    def _combine_chunks(self, chunks, crossfade_ms=50):
        """
        Combine audio chunks using pydub crossfade.
        
        Args:
            chunks: List of (audio_np, sr) tuples
            crossfade_ms: Crossfade duration in ms
        
        Returns:
            Path to combined audio file
        """
        if not chunks:
            return None

        combined = None
        
        for audio_np, sr in chunks:
            # Convert numpy to temporary WAV file for pydub
            fd, tmp_path = tempfile.mkstemp(suffix=".wav")
            os.close(fd)
            
            try:
                sf.write(tmp_path, audio_np, sr)
                seg = AudioSegment.from_wav(tmp_path)
                
                if combined is None:
                    combined = seg
                else:
                    # Append with crossfade
                    combined = combined.append(seg, crossfade=crossfade_ms)
            finally:
                if os.path.exists(tmp_path):
                    os.remove(tmp_path)

        if combined is None:
            return None

        # Normalize and save
        combined = effects.normalize(combined)
        
        # Save to temp file
        fd, output_path = tempfile.mkstemp(suffix=".wav")
        os.close(fd)
        combined.export(output_path, format="wav")
        
        return output_path

    def synthesize(self, text, output_path, language=None):
        """
        Synchronous single synthesis (for backward compatibility).
        """
        if language is None:
            language = self.speaker_language
        
        try:
            result = self._synthesize_chunk(text)
            if result is None:
                return None
            
            audio_np, sr = result
            sf.write(output_path, audio_np, sr)
            return output_path
        except Exception as e:
            print(f"[TTS] Synthesis failed: {e}")
            return None


if __name__ == "__main__":
    # Quick test
    tt = TTSModule(
        speaker_reference_path="tests/My test speech_xtts_speaker.wav",
        speaker_language="en",
        skip_warmup=False
    )
    
    chunks = [
        "Hello, this is the first chunk.",
        "This is the second chunk.",
        "And this is the third and final chunk."
    ]
    
    import asyncio
    out = asyncio.run(tt.synthesize_chunks_concurrently(chunks, crossfade_ms=50))
    print(f"Output: {out}")







    # In pipeline_orchestrator.py - Update these sections:

class PipelineOrchestrator:
    def __init__(self, source_lang="en", target_lang="sk"):
        self.source_lang = source_lang
        self.target_lang = target_lang

        print(f"[PIPELINE] Initializing for {source_lang} -> {target_lang}...")
        
        # STT Module
        self.stt_module = STTModule(
            model_size=WHISPER_MODEL_SIZE,
            device=WHISPER_DEVICE,
            compute_type=WHISPER_COMPUTE_TYPE
        )
        
        # MT Module
        mt_device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.mt_module = MTModule(
            source_lang=source_lang,
            target_lang=target_lang,
            device=mt_device
        )
        
        # TTS Module - with skip_warmup=True to save ~4 seconds
        self.tts_module = TTSModule(
            model_name=XTTS_MODEL_NAME,
            speaker_reference_path=SPEAKER_REFERENCE_PATH_WAV,
            speaker_language="cs" if self.target_lang == "sk" else self.target_lang,
            device=TTS_DEVICE,
            compute_type=TTS_COMPUTE_TYPE,
            skip_warmup=True  # Skip warmup to save time
        )

        # Queues
        self.stt_output_queue = asyncio.Queue()
        self.tts_input_queue = asyncio.Queue()
        self.tts_output_queue = asyncio.Queue()

        # Timing
        self.timing_info = {
            "total_pipeline_time": 0.0,
            "startup_time": 0.0,
            "whisper_load": 0.0,
            "mt_load": 0.0,
            "tts_load_global": 0.0,
            "stt_time": 0.0,
            "mt_time": 0.0,
            "tts_time": 0.0,
        }
        
        self.all_transcribed_text = []
        self.all_translated_text = []

    async def _mt_worker(self):
        """Worker for Machine Translation."""
        while True:
            transcribed_text, segments = await self.stt_output_queue.get()
            if transcribed_text is None:
                await self.tts_input_queue.put((None, None))
                break
            
            start_time = time.time()
            translated_text = self.mt_module.translate(transcribed_text)
            translated_text = self.mt_module._fix_slovak_grammar(translated_text)
            end_time = time.time()
            
            self.timing_info["mt_time"] += (end_time - start_time)
            self.all_translated_text.append(translated_text)
            
            print(f"[MT] Translated in {end_time - start_time:.3f}s")
            await self.tts_input_queue.put((translated_text, segments))

    async def _tts_worker(self):
        """Worker for Text-to-Speech with smart chunking."""
        while True:
            translated_text, original_segments = await self.tts_input_queue.get()
            if translated_text is None:
                print("[TTS] Worker stopping.")
                break

            print(f"[TTS] Processing: '{translated_text[:50]}...'")
            start_time = time.time()
            
            # Smart chunking: split by sentences, not arbitrary length
            text_chunks = self._smart_split_text(translated_text)
            print(f"[TTS] Split into {len(text_chunks)} chunks")

            # Synthesize concurrently
            combined_audio_path = await self.tts_module.synthesize_chunks_concurrently(
                text_chunks,
                crossfade_ms=50
            )
            
            end_time = time.time()
            self.timing_info["tts_time"] += (end_time - start_time)
            
            if combined_audio_path:
                print(f"[TTS] Synthesized in {end_time - start_time:.3f}s")
                await self.tts_output_queue.put(combined_audio_path)
            else:
                print("[TTS] Synthesis failed, skipping.")
        
        await self.tts_output_queue.put(None)

    def _smart_split_text(self, text, max_chars=150):
        """
        Split text into chunks respecting sentence boundaries.
        Avoids misalignment issues.
        """
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current = ""
        
        for sentence in sentences:
            if len(current) + len(sentence) + 1 > max_chars and current:
                chunks.append(current.strip())
                current = sentence + " "
            else:
                current += sentence + " "
        
        if current:
            chunks.append(current.strip())
        
        return chunks if chunks else [text]

    # ... rest of your methods remain the same until run_pipeline ...