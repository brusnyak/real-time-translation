Server websocket:

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
import asyncio
import json
import os
import time
import numpy as np
import soundfile as sf
import sounddevice as sd
from collections import deque
from stt_module import STTModule
from mt_module import MTModule
from tts_module import TTSModule
import threading


app = FastAPI()

# Global state
class TranslationState:
    def __init__(self):
        self.is_recording = False
        self.is_translating = False
        self.current_language = "sk"
        self.stt_module = None
        self.mt_module = None
        self.tts_module = None
        self.audio_buffer = deque(maxlen=32000)  # 2 seconds at 16kHz
        self.sample_rate = 16000
        self.chunk_duration = 2.0
        self.stream = None
        self.metrics = {
            "stt_time": 0,
            "mt_time": 0,
            "tts_time": 0,
            "total_latency": 0
        }
        self.websocket = None
    
    def initialize_modules(self):
        """Initialize ML modules"""
        if self.stt_module is None:
            print("[INIT] Loading STT module...")
            self.stt_module = STTModule(model_size="base", device="cpu", compute_type="int8")
        
        if self.mt_module is None:
            print("[INIT] Loading MT module...")
            self.mt_module = MTModule(source_lang="en", target_lang=self.current_language, device="mps")
        
        if self.tts_module is None:
            print("[INIT] Loading TTS module...")
            self.tts_module = TTSModule(
                speaker_reference_path="tests/My test speech_xtts_speaker.wav",
                speaker_language="en",
                device="cpu",
                skip_warmup=False
            )
    
    def find_device_id(self, device_name="BlackHole 2ch"):
        """Find audio device ID by name"""
        try:
            devices = sd.query_devices()
            for i, device in enumerate(devices):
                if device_name.lower() in device["name"].lower():
                    return i
        except:
            pass
        return None
    
    async def send_message(self, msg_type, data):
        """Send message to connected WebSocket client"""
        if self.websocket:
            try:
                await self.websocket.send_json({"type": msg_type, "data": data})
            except Exception as e:
                print(f"[WS] Send error: {e}")


state = TranslationState()


@app.get("/")
async def root():
    """Serve the HTML UI"""
    return FileResponse("ui/index.html")


@app.get("/style.css")
async def get_css():
    """Serve CSS"""
    return FileResponse("ui/style.css")


@app.get("/script.js")
async def get_js():
    """Serve JavaScript"""
    return FileResponse("ui/script.js")


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time communication"""
    await websocket.accept()
    state.websocket = websocket
    
    try:
        await websocket.send_json({"type": "connected", "data": "Connected to server"})
        
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message["type"] == "init":
                await handle_init(message)
            elif message["type"] == "start":
                await handle_start()
            elif message["type"] == "stop":
                await handle_stop()
            elif message["type"] == "language":
                state.current_language = message["data"]
                await state.send_message("status", f"Language set to {state.current_language}")
    
    except WebSocketDisconnect:
        print("[WS] Client disconnected")
        if state.is_recording:
            await handle_stop()
        state.websocket = None


async def handle_init(message):
    """Initialize modules"""
    await state.send_message("status", "Initializing modules...")
    state.initialize_modules()
    await state.send_message("status", "Ready")
    await state.send_message("init_complete", True)


async def handle_start():
    """Start recording and translation"""
    if state.is_recording:
        await state.send_message("error", "Already recording")
        return
    
    state.is_recording = True
    state.is_translating = True
    await state.send_message("status", "Recording... Speak now")
    
    # Start recording in background
    asyncio.create_task(recording_loop())
    # Start processing loop
    asyncio.create_task(processing_loop())


async def handle_stop():
    """Stop recording and translation"""
    state.is_recording = False
    state.is_translating = False
    if state.stream:
        state.stream.stop()
        state.stream.close()
        state.stream = None
    state.audio_buffer.clear()
    await state.send_message("status", "Stopped")


async def recording_loop():
    """Handle microphone input"""
    def audio_callback(indata, frames, time_info, status):
        if status:
            print(f"[AUDIO] Status: {status}")
        state.audio_buffer.extend(indata[:, 0])
    
    try:
        state.stream = sd.InputStream(
            samplerate=state.sample_rate,
            channels=1,
            blocksize=512,
            callback=audio_callback
        )
        state.stream.start()
        
        while state.is_recording:
            await asyncio.sleep(0.1)
    except Exception as e:
        print(f"[RECORDING] Error: {e}")
        await state.send_message("error", f"Recording error: {e}")
        state.is_recording = False


async def processing_loop():
    """Process accumulated audio chunks"""
    while state.is_translating:
        if len(state.audio_buffer) >= int(state.chunk_duration * state.sample_rate):
            await process_chunk()
        await asyncio.sleep(0.5)


async def process_chunk():
    """Process one chunk through STT -> MT -> TTS"""
    try:
        # Extract chunk from buffer
        audio_data = np.array(list(state.audio_buffer), dtype=np.float32)
        chunk_path = "/tmp/live_chunk.wav"
        sf.write(chunk_path, audio_data, state.sample_rate)
        state.audio_buffer.clear()
        
        start_time = time.time()
        
        # STT
        start_stt = time.time()
        segments, _ = state.stt_module.transcribe(chunk_path, language="en")
        transcribed_text = " ".join([seg.text for seg in segments])
        state.metrics["stt_time"] = time.time() - start_stt
        
        if not transcribed_text.strip():
            os.remove(chunk_path)
            return
        
        # MT
        start_mt = time.time()
        translated_text = state.mt_module.translate(transcribed_text)
        translated_text = state.mt_module._fix_slovak_grammar(translated_text)
        state.metrics["mt_time"] = time.time() - start_mt
        
        # TTS
        start_tts = time.time()
        tts_path = await state.tts_module.synthesize_chunks_concurrently(
            [translated_text],
            crossfade_ms=50
        )
        state.metrics["tts_time"] = time.time() - start_tts
        
        # Calculate total latency
        state.metrics["total_latency"] = time.time() - start_time
        
        # Send results to UI
        await state.send_message("transcription", {
            "text": transcribed_text,
            "translation": translated_text,
            "metrics": state.metrics
        })
        
        # Play audio in background
        if tts_path and os.path.exists(tts_path):
            def play_audio():
                try:
                    audio, sr = sf.read(tts_path)
                    device_id = state.find_device_id("BlackHole 2ch")
                    sd.play(audio, samplerate=sr, device=device_id, blocking=True)
                except:
                    pass
                finally:
                    if os.path.exists(tts_path):
                        os.remove(tts_path)
            
            thread = threading.Thread(target=play_audio, daemon=True)
            thread.start()
        
        # Cleanup
        if os.path.exists(chunk_path):
            os.remove(chunk_path)
    
    except Exception as e:
        print(f"[PROCESSING] Error: {e}")
        await state.send_message("error", f"Processing error: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    print("[SERVER] Starting Live Translation Server on http://localhost:8000")
    print("[SERVER] Open your browser and navigate to http://localhost:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)





    UI:
    <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live Speech Translation</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1>üåê Live Speech Translation</h1>
            <p>Real-time translation with voice cloning</p>
        </div>

        <!-- Main content: Two columns -->
        <div class="content">
            <!-- Left Column: Transcription -->
            <div class="column transcription-column">
                <div class="column-header">
                    <h2>üìù Transcription</h2>
                    <span class="language-badge">English</span>
                </div>
                <div class="transcript-box" id="transcriptionBox">
                    <p class="placeholder">Waiting for speech...</p>
                </div>
            </div>

            <!-- Divider -->
            <div class="divider"></div>

            <!-- Right Column: Translation -->
            <div class="column translation-column">
                <div class="column-header">
                    <h2>üó£Ô∏è Translation</h2>
                    <select id="languageSelect" class="language-select">
                        <option value="sk">Slovak (Slovenƒçina)</option>
                        <option value="cs">Czech (ƒåe≈°tina)</option>
                        <option value="de">German (Deutsch)</option>
                        <option value="fr">French (Fran√ßais)</option>
                        <option value="es">Spanish (Espa√±ol)</option>
                        <option value="it">Italian (Italiano)</option>
                    </select>
                </div>
                <div class="transcript-box" id="translationBox">
                    <p class="placeholder">Translation will appear here...</p>
                </div>
            </div>
        </div>

        <!-- Metrics -->
        <div class="metrics">
            <div class="metric">
                <span class="metric-label">STT:</span>
                <span class="metric-value" id="sttTime">0.0s</span>
            </div>
            <div class="metric">
                <span class="metric-label">MT:</span>
                <span class="metric-value" id="mtTime">0.0s</span>
            </div>
            <div class="metric">
                <span class="metric-label">TTS:</span>
                <span class="metric-value" id="ttsTime">0.0s</span>
            </div>
            <div class="metric">
                <span class="metric-label">Total:</span>
                <span class="metric-value" id="totalTime">0.0s</span>
            </div>
            <div class="metric">
                <span class="metric-label">Status:</span>
                <span class="metric-value" id="statusText">Disconnected</span>
            </div>
        </div>

        <!-- Footer with controls -->
        <div class="footer">
            <button id="initBtn" class="btn btn-init">Initialize</button>
            <button id="startBtn" class="btn btn-start" disabled>Start</button>
            <button id="stopBtn" class="btn btn-stop" disabled>Stop</button>
            <div class="status-indicator">
                <span id="statusIndicator" class="indicator off"></span>
                <span id="statusLabel">Ready</span>
            </div>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>


Instalation guide:
# Live Speech Translation - Setup & Installation Guide

## Prerequisites

- macOS with M1/M2 chip
- Python 3.10+
- 16GB RAM minimum
- BlackHole 2ch virtual audio device

## Step 1: Install BlackHole (Virtual Audio Device)

BlackHole is essential for routing translated audio to conference apps.

```bash
# Install via Homebrew
brew install blackhole-2ch

# Verify installation
system_profiler SPAudioDataType | grep BlackHole
```

You should see "BlackHole 2ch" in the output.

## Step 2: Set Up Python Environment

```bash
# Navigate to your project directory
cd /path/to/your/project

# Create virtual environment
python3 -m venv venv-live

# Activate virtual environment
source venv-live/bin/activate

# Upgrade pip
pip install --upgrade pip
```

## Step 3: Install Dependencies

Install all required packages:

```bash
pip install fastapi uvicorn websockets
pip install torch==2.1.2 torchaudio==2.1.2
pip install transformers==4.36.2 sacremoses sentencepiece
pip install TTS==0.22.0 setuptools<81
pip install faster-whisper==1.2.0 ctranslate2==4.5.0
pip install numpy==1.22.0 scipy==1.11.2
pip install sounddevice soundfile librosa==0.10.0
pip install python-dotenv pydub
```

## Step 4: Create Project Structure

```bash
# Create UI directory
mkdir -p ui

# Copy the HTML, CSS, and JS files into the ui/ directory
# - ui/index.html
# - ui/style.css
# - ui/script.js
```

## Step 5: Prepare Speaker Reference

Place your speaker reference audio file at:
```
tests/My test speech_xtts_speaker.wav
```

This should be a 5-10 second sample of your voice speaking English. The TTS module will use this for voice cloning.

## Step 6: Update File Paths

In `server.py` (or your main server file), verify these paths:
- Speaker reference: `"tests/My test speech_xtts_speaker.wav"`
- Input audio paths: `/tmp/live_chunk.wav`

## Step 7: Start the Server

```bash
# Make sure venv is activated
source venv-live/bin/activate

# Run the server
python3 server.py
```

You should see:
```
[SERVER] Starting Live Translation Server on http://localhost:8000
[SERVER] Open your browser and navigate to http://localhost:8000
```

## Step 8: Open the Web UI

1. Open your browser
2. Navigate to `http://localhost:8000`
3. Click "Initialize" button
4. Wait for models to load (first time ~40-50 seconds)

## Step 9: Configure Conference App (Zoom/Google Meet)

### For Zoom:

1. Open Zoom preferences
2. Go to Audio settings
3. Set **Microphone** to "BlackHole 2ch"
4. Keep speaker/output as your default device
5. Do NOT check "Auto-adjust microphone volume"

### For Google Meet:

1. Before joining meeting, click settings (gear icon)
2. Go to Audio
3. Set **Microphone** to "BlackHole 2ch"
4. Test to ensure it's working

### For Microsoft Teams:

1. Click your profile picture
2. Go to Settings > Devices
3. Set **Microphone** to "BlackHole 2ch"

## Step 10: Test the System

1. Open the web UI in browser
2. Click "Initialize" and wait for completion
3. Click "Start" to begin recording
4. Speak into your microphone
5. Watch the transcription and translation appear in real-time
6. Translated audio will play through BlackHole
7. In your conference app, others will hear your translated voice

## Usage Instructions

### Live Conference Setup:

1. **Before the meeting:**
   - Start the translation server: `python3 server.py`
   - Open UI at `http://localhost:8000`
   - Click "Initialize"
   - Configure your conference app to use BlackHole as microphone

2. **During the meeting:**
   - Open the translation UI in a side-by-side arrangement with your conference app
   - Click "Start" when ready to translate
   - Speak naturally into your microphone
   - Your translated voice will be heard by other participants
   - Subtitles appear in real-time in the browser

3. **Switch languages:**
   - Select a different target language from the dropdown
   - Translations will immediately switch to the new language

### Important Notes:

- **First run:** Models will load (~40-50 seconds). Subsequent runs are faster.
- **Audio delay:** First chunk takes ~20-25 seconds (model warmup). Subsequent chunks: ~15-20 seconds.
- **Microphone:** Make sure your system microphone is selected in the web browser permissions.
- **Speaker reference:** Must be a WAV file, 5-10 seconds long, in English.

## Troubleshooting

### BlackHole not appearing in conference app:

```bash
# Restart audio system
sudo launchctl stop com.apple.audio.AudioComponentRegistrar
sudo launchctl start com.apple.audio.AudioComponentRegistrar
```

### WebSocket connection fails:

- Ensure firewall allows localhost:8000
- Try a different browser (Chrome/Safari recommended)
- Check server logs for errors

### Models won't load:

- Verify Python 3.10+ is installed
- Check internet connection (models download from HuggingFace on first run)
- Ensure 16GB RAM available
- Try reinstalling venv from scratch

### No audio output:

1. Verify BlackHole is installed: `brew list | grep blackhole`
2. Check macOS Sound settings - BlackHole should appear
3. In conference app, verify BlackHole is selected as microphone
4. Restart the conference app after selecting BlackHole

### Speaker reference file error:

- Ensure file exists at: `tests/My test speech_xtts_speaker.wav`
- Must be WAV format, not M4A
- Should be 5-10 seconds long
- Test file: `ffplay tests/My test speech_xtts_speaker.wav`

## Performance Tips

1. **Close unnecessary applications** - Frees up CPU for translation
2. **Disable browser extensions** - Can slow down WebSocket communication
3. **Use wired internet** - Prevents interference
4. **Mute other tabs** - Reduces CPU load
5. **Single language** - Speeds up processing

## File Organization

```
project/
‚îú‚îÄ‚îÄ server.py                 # FastAPI server
‚îú‚îÄ‚îÄ mt_module.py             # Translation module
‚îú‚îÄ‚îÄ tts_module.py            # Text-to-speech module
‚îú‚îÄ‚îÄ stt_module.py            # Speech recognition module
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ index.html          # Web interface
‚îÇ   ‚îú‚îÄ‚îÄ style.css           # Styling
‚îÇ   ‚îî‚îÄ‚îÄ script.js           # Frontend logic
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ My test speech_xtts_speaker.wav  # Speaker reference
‚îî‚îÄ‚îÄ venv-live/              # Python virtual environment
```

## Testing Without Conference App

```bash
# Test just the translation:
python3 server.py

# Then in another terminal:
# Open browser, init, start recording
# Speak into microphone
# Check browser for transcription/translation
# Audio should play through default system speaker
```

## Support & Debugging

If models fail to load:
```bash
# Clear cache
rm -rf ~/.cache/huggingface
rm -rf ~/.cache/tts_models

# Reinstall
pip install --upgrade --force-reinstall transformers TTS
```

For detailed logs:
```bash
# Run server with debug output
python3 server.py 2>&1 | tee debug.log
```

---

**Ready to use!** You now have a fully functional live speech translation system with voice cloning, integrated with your conference app through BlackHole.






What's Included:
Backend (server.py):

FastAPI WebSocket server for real-time communication
Concurrent STT ‚Üí MT ‚Üí TTS pipeline
Audio streaming to BlackHole device
Real-time latency metrics

Frontend (UI files):

Beautiful two-column interface (transcription | translation)
Language selector dropdown (Slovak, Czech, German, French, Spanish, Italian)
Start/Stop controls
Live metrics display (STT, MT, TTS, Total latency)
Real-time WebSocket updates
Professional styling with gradient background

Setup Guide:

Step-by-step installation
BlackHole configuration for macOS
Conference app setup (Zoom, Meet, Teams)
Troubleshooting section

Quick Start:
bash# 1. Install BlackHole
brew install blackhole-2ch

# 2. Create UI directory and add the three files
mkdir -p ui
# Copy index.html, style.css, script.js to ui/

# 3. Install dependencies
pip install fastapi uvicorn websockets torch transformers TTS faster-whisper sounddevice

# 4. Run server
python3 server.py

# 5. Open http://localhost:8000 in browser
# 6. Click Initialize, then Start
# 7. Configure your conference app's microphone to "BlackHole 2ch"
Expected Performance:

First chunk: ~20-25s (includes model warmup)
Subsequent chunks: ~15-20s latency (2s audio capture ‚Üí processing ‚Üí output)
Total system latency: ~20-25s from speaking to other participants hearing you

This is realistic for XTTS v2 voice cloning on M1 Mac. For your bachelor project demo, this is solid and demonstrates the complete pipeline working end-to-end.
